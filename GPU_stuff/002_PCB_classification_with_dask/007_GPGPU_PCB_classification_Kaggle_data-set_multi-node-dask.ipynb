{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import gc # garbage collector to free GPU memory\n",
    "from tqdm import tqdm # visualization of progress in loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c13d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "#   adding some commands for multi-GPU utilization\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait\n",
    "\n",
    "# boundary conditions to ristric using Teslas\n",
    "n_workers = 2 # restricting the usage of the GPU to avoid utilizing the Quadro P4000\n",
    "n_streams = 8 # Performance optimization - has been 8\n",
    "\n",
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(n_workers= n_workers, threads_per_worker=1)\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "\n",
    "print(\"which worker will be used: \", workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292c125-9a73-4416-81a3-50861517c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading kaggle API to download the data set needed\n",
    "import kaggle\n",
    "\n",
    "dataset_name= \"micropcb-images.zip\" # name of the zip-file, which is getting downloaded from kaggle\n",
    "dataset_base_folder= \"/media/igor/FEM-storage-Linux/Phython_project_datasets\"\n",
    "dataset_destination= dataset_base_folder+ \"/PCB_defects_dataset/\"\n",
    "notebook_location= \"/media/igor/Linux_drive-A/Eigene_Dateien/Python_project/pythonProject-007_GPGPUs\"\n",
    "\n",
    "check_zip=0  # control variable set= 1 if zip-file already has been downloaded\n",
    "check_data=0 # control variable set= 1 if dataset already has been extracted from zip-file\n",
    "\n",
    "print(\"1.1 looping through the notebook directory to check if the dataset has been downloaded already\")\n",
    "for file_path in os.listdir(notebook_location): # looping through notebook directory to check if dataset has been downloaded already\n",
    "    print(file_path)\n",
    "    if os.path.isfile(os.path.join(notebook_location, file_path)):\n",
    "        if file_path == dataset_name: # compare the current file name with the deisred one\n",
    "            print(\"*** dataset exists ***\")\n",
    "            check_zip=1\n",
    "            break\n",
    "if check_zip==0:\n",
    "    print(\"\\n\".join((\"dataset is not yet found in notebook directory\", \"checking dataset destination for extracted files...\")))\n",
    "\n",
    "print(\"2.1 checking if dataset has been extracted already to file destination\")\n",
    "for file_path in os.listdir(dataset_base_folder): # looping through the file destination in order to check if they have been extracted already\n",
    "    if os.path.isdir(os.path.join(dataset_base_folder, file_path)):\n",
    "        print(file_path)\n",
    "        if file_path==dataset_destination.split(\"/\")[-2]:\n",
    "            print(\"*** files are already extracted from zip-file ***\")\n",
    "            check_data=1\n",
    "if check_data==0:\n",
    "    print(\"\\n\".join((\"dataset is not yet found in file destination\", \"files are now either extracted or downloaded first and then extracted to its destination\")))\n",
    "\n",
    "\n",
    "# downloading the dataset from kaggle\n",
    "if check_zip==0 and check_data==0:\n",
    "    print(\"2.2 start downloading the dataset from kaggle\")\n",
    "    # !kaggle datasets download -d dataset_lacation\n",
    "    check_zip=1\n",
    "\n",
    "# generating a folder on the local machine and unzipping the files\n",
    "if check_data==0:\n",
    "    print(\"2.3 generating the file destination folder: \", dataset_destination)\n",
    "    # os.makedirs(dataset_destination, exist_ok=True)\n",
    "\n",
    "# unzipping the files\n",
    "if check_zip==1 and check_data==0:\n",
    "    print(\"2.4 extracting the dataset to its file destination\")\n",
    "    # !unzip micropcb-images.zip -d /media/igor/FEM-storage-Linux/Phython_project_datasets/PCB_defects_dataset\n",
    "    check_data=1\n",
    "\n",
    "# remoev zip-file after extracting the data\n",
    "if check_zip==1 and check_data==1:\n",
    "    print(\"2.5 removing the zip-file from notebook directory to free disk space\")\n",
    "    #os.remove(dataset_name) # the zip-file needs to be in the same folder as the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b621aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some basic settings\n",
    "if check_data==1:\n",
    "    print(\"dataset location is on SSD\")\n",
    "    directory_settings= dataset_destination\n",
    "else:\n",
    "    print(\"dataset location is on HDD\")\n",
    "    directory_settings= \"/media/igor/Linux_drive-A/Eigene_Dateien/Python_project/pythonProject-007_GPGPUs/data-sets/PCB_dataset_defects/\"\n",
    "# directory_settings= \"/home/igor/Eigene_Dateien/Python_project/pythonProject-007_GPGPUs/data-sets/PCB_dataset_defects/\"\n",
    "\n",
    "directory_train= directory_settings + \"train_coded/train_coded\"\n",
    "directory_test=  directory_settings + \"test_coded/test_coded\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88489a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some helper-functions\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# reading a folder and returning its file names\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "def file_names(location):\n",
    "    return os.listdir(location)\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# generating a test and training DataFrame\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "def df_gen(_type):\n",
    "    DataFrame_gen= 0\n",
    "    for i, name in enumerate(file_names(directory_settings)):\n",
    "        print(\"currently checking: \", name)\n",
    "        if (name[-3:]== \"csv\") & (DataFrame_gen==0) & (name.split(\"_\")[0]==_type):\n",
    "            print(\"generating DataFrame....\")\n",
    "            df= pd.read_csv(directory_settings + name)\n",
    "            DataFrame_gen=1\n",
    "            continue\n",
    "        if (name[-3:]== \"csv\") & (DataFrame_gen==1) & (name.split(\"_\")[0]==_type):\n",
    "            print(f\"...adding {name} to my DataFrame \")\n",
    "            df_temp= pd.read_csv(directory_settings + name)\n",
    "            df_temp=df_temp.drop(\"Image\", axis=1)\n",
    "            df= pd.concat([df, df_temp], axis=1)\n",
    "        else:\n",
    "            print(\"jumping to next file...\")\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# getting some additional info from the file-name codings\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "def dummy_gen(df, array, char0, char1, char2, char3):\n",
    "    for i, name in enumerate(array):\n",
    "        char= vars()[\"char\"+ str(i)] # changing the character dictianory to the ones supplied above\n",
    "        df[name]= df[\"Image\"].apply(lambda x: char[x[i]])\n",
    "\n",
    "    return df\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::::::::::: garbage collector ::::::::::::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "'''https://github.com/rapidsai/cuml/issues/5769'''\n",
    "'''     thanks to: immanuelazn     '''\n",
    "class GarbageCollector(): # BaseEstimator, TransformerMixin\n",
    "    # this will free GPU memory after each itereation in GridSearchCV\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y = None):\n",
    "        gc.collect()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        gc.collect()\n",
    "        return X\n",
    "\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# :::::::::::::: background detection ::::::::::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "def background(img, threshold, pixel): # common values threshold= 160, pixel=35\n",
    "    # imArray= cv2.imread(img)\n",
    "    # color conversion\n",
    "    imArray= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # adaptive background definition of the Gray-scale image\n",
    "    return cv2.adaptiveThreshold(imArray, threshold, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,pixel,5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca02700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading my definitions into a DataFrame\n",
    "\n",
    "df_train= df_gen(\"train\")\n",
    "df_test=  df_gen(\"test\")\n",
    "\n",
    "# changing two identical column names\n",
    "df_train.columns= ['Image', 'Angle', 'PCB_Left', 'PCBTop', 'PCB_Width', 'PCB_Height',\n",
    "                   'Ratio_of_T-2-B_Edge_Length', 'Img_Width','Img_Height']\n",
    "df_test.columns= ['Image', 'Angle', 'PCB_Left', 'PCBTop', 'PCB_Width', 'PCB_Height',\n",
    "                  'Ratio_of_T-2-B_Edge_Length', 'Img_Width','Img_Height']\n",
    "\n",
    "# reverse engineering the file-name-coded specs\n",
    "# origin is the read-me text file\n",
    "_1st_char= {\"A\": \"Raspberry Pi A+\", \"B\": \"Arduino Mega 2560 (Blue)\", \"C\":\"Arduino Mega 2560 (Black)\", \n",
    "                \"D\": \"Arduino Mega 2560 (Black and Yellow)\", \"E\": \"Arduino Due\", \"F\": \"Beaglebone Black\",\n",
    "                \"G\": \"Arduino Uno (Green)\", \"H\": \"Raspberry Pi 3 B+\", \"I\": \"Raspberry Pi 1 B+\", \n",
    "                \"J\": \"Arduino Uno Camera Shield\", \"K\": \"Arduino Uno (Black)\", \"L\": \"Arduino Uno WiFi Shield\",\n",
    "                \"M\": \"Arduino Leonardo\"}\n",
    "char_1= {\"A\": 0, \"B\": 1, \"C\":2, \"D\": 3, \"E\": 4, \"F\": 5,\"G\": 6, \"H\": 7, \"I\": 8,\"J\": 9, \"K\": 10, \"L\": 11,\"M\": 12}\n",
    "_2nd_char= {\"A\": \"Wide left rotation\", \"B\": \"Shallow left rotation\", \"C\": \"Neutral rotation\",\n",
    "                \"D\": \"Shallow right rotation\", \"E\": \"Wide right rotation\"}\n",
    "\n",
    "_3rd_char= {\"A\": \"12 inches left of the camera position\", \"B\": \"6 inches left of the camera position\",\n",
    "                \"C\": \"centered horizontally relative to the camera\", \"D\": \"6 inches right of the camera position\",\n",
    "                \"E\": \"12 inches right of the camera position\"}\n",
    "_4th_char= _3rd_char\n",
    "_5th_char= {\"1\": \"Images numbered 1-4 are train images\", \"2\": \"Images numbered 1-4 are train images\",\n",
    "                \"3\": \"Images numbered 1-4 are train images\", \"4\": \"Images numbered 1-4 are train images\",\n",
    "                \"5\": \"Images numbered 5 are test images\"}\n",
    "char_def= [\"model\", \"totation_type\", \"x-coord._capture\", \"y-coord._capture\"]\n",
    "\n",
    "\n",
    "print(\"including the specs from the file-name-coding into my DataFrame\")\n",
    "df_train= dummy_gen(df_train, char_def, _1st_char, _2nd_char, _3rd_char, _4th_char)\n",
    "df_test=  dummy_gen(df_test, char_def, _1st_char, _2nd_char, _3rd_char, _4th_char)\n",
    "\n",
    "print(\"::::::::::::::::::::::::::::::::\")\n",
    "print(\":::::::::  finished  :::::::::::\")\n",
    "print(\"::::::::::::::::::::::::::::::::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809b111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get some dummy values from Pandas\n",
    "df_train_dummy= pd.get_dummies(df_train.drop(\"Image\", axis=1), drop_first=True)\n",
    "df_train_dummy[\"Image\"]= df_train[\"Image\"]\n",
    "df_test_dummy=  pd.get_dummies(df_test.drop(\"Image\", axis=1), drop_first=True)\n",
    "df_test_dummy[\"Image\"]= df_test[\"Image\"]\n",
    "\n",
    "df_train_dummy.head()\n",
    "\n",
    "print(\"::::::::::::::::::::::::::::::::\")\n",
    "print(\":::::::::  finished  :::::::::::\")\n",
    "print(\"::::::::::::::::::::::::::::::::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aaa29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::::::::::: dictionary generation :::::::::::::::::\n",
    "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "print(\"generating file locations for my train dataset\")\n",
    "file_names_train, file_names_test = np.array([]) ,np.array([])\n",
    "pbar = tqdm(file_names(directory_train), total=len(file_names(directory_train)))\n",
    "for i, img_name in enumerate(pbar):\n",
    "    file_names_train= np.append(file_names_train, directory_train+ \"/\"+ file_names(directory_train)[i])\n",
    "\n",
    "    \n",
    "print(\"generating file locations for my test dataset\")\n",
    "pbar = tqdm(file_names(directory_test), total=len(file_names(directory_test)))\n",
    "for i, img_name in enumerate(pbar):\n",
    "    file_names_test= np.append(file_names_test, directory_test+ \"/\"+ file_names(directory_test)[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d266c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# :::::::::::::::: some exploratory DA ::::::::::::::::::\n",
    "# :::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "print(\"analizing image dimensions and total pixel count for train DS\")\n",
    "\n",
    "picture_stats_train, picture_stats_test= np.array([]), np.array([])\n",
    "\n",
    "# train data set (DS)\n",
    "pbar = tqdm(file_names_train, total=len(file_names_train))\n",
    "for img_name in pbar:\n",
    "    image_data= cv2.imread(img_name)\n",
    "    X, y = image_data.shape[0], image_data.shape[1]\n",
    "    picture_stats_train= np.append(picture_stats_train, [X, y, X* y])\n",
    "\n",
    "picture_stats_train= picture_stats_train.reshape(len(file_names_train), 3)\n",
    "df_stats_train= pd.DataFrame(picture_stats_train, columns=[\"pixel_X\", \"pixel_y\", \"total_pixel\"])\n",
    "\n",
    "print(\"analizing image dimensions and total pixel count for test DS\")\n",
    "# test data set (DS)\n",
    "pbar = tqdm(file_names_test, total=len(file_names_test))\n",
    "for img_name in pbar:\n",
    "    image_data= cv2.imread(img_name)\n",
    "    X, y = image_data.shape[0], image_data.shape[1]\n",
    "    picture_stats_test= np.append(picture_stats_test, [X, y, X* y])\n",
    "\n",
    "picture_stats_test= picture_stats_test.reshape(len(file_names_test), 3)\n",
    "df_stats_test= pd.DataFrame(picture_stats_test, columns=[\"pixel_X\", \"pixel_y\", \"total_pixel\"])\n",
    "\n",
    "# visualization of the obtained information about the pictures in my data set\n",
    "fig, axn= plt.subplots(1,2, figsize=(10,5))\n",
    "sns.pointplot(data=df_stats_train, x= 1, y=\"pixel_X\", errorbar= \"sd\", label= \"pixel count X-axis (train DS)\", ax= axn[0])\n",
    "sns.pointplot(data=df_stats_train, x= 2, y=\"pixel_y\", errorbar= \"sd\", label= \"pixel count y-axis (train DS)\", ax= axn[0])\n",
    "sns.histplot(data=df_stats_train, x=\"total_pixel\", label= \"picture resolution (train DS)\", ax= axn[1])\n",
    "\n",
    "sns.pointplot(data=df_stats_test, x= 1.2, y=\"pixel_X\", errorbar= \"sd\", marker=\"D\", label= \"pixel count X-axis (test DS)\", ax= axn[0])\n",
    "sns.pointplot(data=df_stats_test, x= 2.2, y=\"pixel_y\", errorbar= \"sd\", marker=\"D\", label= \"pixel count y-axis (test DS)\", ax= axn[0])\n",
    "sns.histplot(data=df_stats_test, x=\"total_pixel\", label= \"picture resolution (test DS)\", ax= axn[1])\n",
    "\n",
    "[axn[i].legend() for i in range(2)]\n",
    "[axn[i].grid(True) for i in range(2)]\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81256e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# :::: visualization of my transformed test image ::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "img_size= 64 # 64\n",
    "pixel= 3\n",
    "threshold= 165\n",
    "\n",
    "image_loc= file_names_train[1]\n",
    "print(\"file name to load: \\n\", image_loc)\n",
    "\n",
    "img= cv2.imread(image_loc)\n",
    "image_red= cv2.resize(img, (img_size, img_size))\n",
    "\n",
    "imArray= cv2.cvtColor(image_red, cv2.COLOR_BGR2GRAY)\n",
    "# adaptive background definition of the Gray-scale image\n",
    "imArray= cv2.adaptiveThreshold(imArray, threshold, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,pixel,5)\n",
    "tmp1= cv2.createBackgroundSubtractorMOG2().apply(imArray)\n",
    "tmp2= imArray\n",
    "tmp3= cv2.createBackgroundSubtractorKNN().apply(image_red)\n",
    "\n",
    "fig, axn= plt.subplots(2,2, figsize=(10,10))\n",
    "axn[0,0].imshow(tmp1, cmap= \"gray\")\n",
    "axn[0,1].imshow(tmp2, cmap= \"gray\")\n",
    "axn[1,0].imshow(img, cmap= \"gray\")\n",
    "axn[1,1].imshow(image_red, cmap= \"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0c30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# :::::::::::::::::::::::::::::::::::::::::::\n",
    "# this is with background subtraction methode\n",
    "# :::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "import cupy as cp\n",
    "import cudf\n",
    "\n",
    "# **************************\n",
    "verbose_01= 1 # can be used to enable an additional contrast picture if set= 1\n",
    "# **************************\n",
    "\n",
    "# stagging my cropped image together with my wavelet transforemd data into a numpy array for model-training\n",
    "versions = 13 # dataset comprises of 13 differenct PCBs\n",
    "lim_train= 500* versions -1  # not the full size yet: len(file_names(directory_train))\n",
    "lim_test = 125* versions -1  # not the full size yet: len(file_names(directory_test))\n",
    "if verbose_01==1: \n",
    "    img_size= 64 # 76\n",
    "    bit= 4\n",
    "else: \n",
    "    img_size=76 # 76\n",
    "    bit=3\n",
    "X_train, y_train= [], []\n",
    "X_test, y_test= [], []\n",
    "\n",
    "# :: X_train_cu, y_train_cu= cp.array([]), cp.array([])\n",
    "# :: X_test_cu, y_test_cu= cp.array([]), cp.array([])\n",
    "\n",
    "pbar = tqdm(file_names_train, total=lim_train)\n",
    "for num,training_image in enumerate(pbar):\n",
    "    # img= cv2.imread(training_image)\n",
    "    img=cv2.resize(cv2.imread(training_image), (img_size,img_size))\n",
    "    if img is None:\n",
    "        continue\n",
    "    scalled_raw_img = img\n",
    "    imArray= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    scalled_img_haar= cv2.adaptiveThreshold(imArray, 165, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,3,5)\n",
    "    # scalled_img_haar= cv2.resize(img_haar, (img_size,img_size))\n",
    "    if verbose_01==1: combined_img= np.vstack((scalled_raw_img.reshape(img_size*img_size*3,1), scalled_img_haar.reshape(img_size*img_size,1)))\n",
    "    else: combined_img= scalled_raw_img.reshape(img_size*img_size*3,1)\n",
    "    X_train.append(combined_img)\n",
    "    y_train.append(df_train[\"Image\"].apply(lambda x: char_1[x[0]])[num]) #y_train.append(df_train[\"Image\"].apply(lambda x: _1st_char[x[0]])[num])\n",
    "    # CUDA version of numpy for GPU inferences\n",
    "    # :: X_train_cu= cp.append(X_train_cu, combined_img)\n",
    "    if num==lim_train: break # needs to be removed for full dataset\n",
    "\n",
    "# simple conversion to CuPy\n",
    "# :: y_train_cu= cudf.Series(y_train).to_cupy()\n",
    "\n",
    "# reshaping my training data\n",
    "y_train= np.array(y_train).astype(np.int32)\n",
    "X_train= np.array(X_train).reshape(len(X_train),int(img_size**2* bit)).astype(np.float32) #.astype(float), with astype(int8) memory gets saved\n",
    "# :: X_train_cu= cp.array(X_train_cu).reshape(num+1, int(img_size**2* bit))\n",
    "print(\"the shape of the data in my train dataset is: \", X_train.shape)\n",
    "\n",
    "\n",
    "pbar = tqdm(file_names_test, total=lim_test)\n",
    "for num,training_image in enumerate(pbar):\n",
    "    # img= cv2.imread(training_image)\n",
    "    img= cv2.resize(cv2.imread(training_image), (img_size,img_size))\n",
    "    if img is None:\n",
    "        continue\n",
    "    scalled_raw_img = img\n",
    "    imArray= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    scalled_img_haar= cv2.adaptiveThreshold(imArray, 165, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,3,5)\n",
    "    # scalled_img_haar= cv2.resize(img_haar, (img_size,img_size))\n",
    "    if verbose_01==1: combined_img= np.vstack((scalled_raw_img.reshape(img_size*img_size*3,1), scalled_img_haar.reshape(img_size*img_size,1)))\n",
    "    else: combined_img= scalled_raw_img.reshape(img_size*img_size*3,1)\n",
    "    X_test.append(combined_img)\n",
    "    y_test.append(df_test[\"Image\"].apply(lambda x: char_1[x[0]])[num])\n",
    "    # CUDA version of numpy for GPU infenrences\n",
    "    # :: X_test_cu= cp.append(X_test_cu, combined_img)\n",
    "    if num==lim_test: break # needs to be removed for full dataset\n",
    "\n",
    "# simple conversion to CuPy\n",
    "# :: y_test_cu= cudf.Series(y_test).to_cupy()\n",
    "\n",
    "# reshaping my training data\n",
    "y_test= np.array(y_test).astype(np.int32)\n",
    "X_test= np.array(X_test).reshape(len(X_test),int(img_size**2 * bit)).astype(np.float32) # .astype(float), with astype(int8) memory gets saved\n",
    "# :: X_test_cu= cp.array(X_test_cu).reshape(num+1, int(img_size**2* bit))\n",
    "print(\"the shape of the data in my test dataset is: \", X_test.shape)\n",
    "\n",
    "print(\"::::::::::::::::::::::::::::::::\")\n",
    "print(\":::::::::  finished  :::::::::::\")\n",
    "print(\"::::::::::::::::::::::::::::::::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::: definition of the distribution to the workers:::\n",
    "# :::::::::::::: sharing the DataFrame :::::::::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "import dask_cudf\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "\n",
    "n_partitions = n_workers\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# :::::::::::::::::::: taken from ::::::::::::::::::::\n",
    "# https://github.com/rapidsai/cuml/blob/main/notebooks/random_forest_mnmg_demo.ipynb\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "def distribute(X, y):\n",
    "    print(\"loading the numpy arrays into Cuda DataFrames\")\n",
    "    # transfering normal DataFrame into cudf and move it to the GPU\n",
    "    X_cudf= cudf.DataFrame(X)\n",
    "    y_cudf= cudf.Series(y)\n",
    "    \n",
    "    print(f\"the shape of my X_cudf is {X_cudf.shape} and from my y_cudf is {y_cudf.shape}\")\n",
    "    print(\"transforming cudf to dask_cudf and sharing it on the GPU\")\n",
    "    # now generate dask DataFrames for parallelization\n",
    "    X_dask= dask_cudf.from_cudf(X_cudf, npartitions= n_partitions)\n",
    "    y_dask= dask_cudf.from_cudf(y_cudf, npartitions= n_partitions)\n",
    "    \n",
    "    # Persist to cache the data in active memory\n",
    "    X_dask, y_dask = \\\n",
    "      dask_utils.persist_across_workers(c, [X_dask, y_dask], workers=workers)\n",
    "    \n",
    "    return X_dask, y_dask\n",
    "\n",
    "X_train_dask, y_train_dask= distribute(X_train, y_train)\n",
    "X_test_dask, y_test_dask= distribute(X_test, y_test)\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32962171-6d9a-4e12-8b45-1d3f8e6066d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84025a-1914-4e58-87fb-e29ca124234f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cuml.metrics import accuracy_score\n",
    "from cuml.dask.ensemble import RandomForestClassifier as cumlDaskRF\n",
    "\n",
    "\n",
    "print(\"definition of the model\")\n",
    "cuml_model = cumlDaskRF(n_estimators=300, n_streams=1, random_state= 1984) # (max_depth=max_depth, n_estimators=n_trees, n_bins=n_bins, n_streams=n_streams)\n",
    "\n",
    "print(\"fitting the model to my data-set\")\n",
    "cuml_model.fit(X_train_dask, y_train_dask)\n",
    "wait(cuml_model.rfs) # Allow asynchronous training tasks to finish\n",
    "\n",
    "# testing the model\n",
    "print(\"validating the model....\")\n",
    "cuml_y_pred = cuml_model.predict(X_test_dask).compute().to_numpy()\n",
    "\n",
    "# Due to randomness in the algorithm, you may see slight variation in accuracies\n",
    "print(f\"CuML accuracy:  {accuracy_score(y_test, cuml_y_pred):.3f}\")\n",
    "\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::\n",
    "# ::::::::::::::::::::::::::::::::\n",
    "\n",
    "print(\"::::::::::::::::::::::::::::::::\")\n",
    "params= [100, 200, 300, 500, 750, 1200] # n_estimators for RandomForestLCassifier\n",
    "res_array= [] # array to store the resutls of my selfmade parameter search\n",
    "\n",
    "def cuml_model_est(i):\n",
    "    return cumlDaskRF(n_estimators= i, n_streams= 1, random_state= 1984)\n",
    "\n",
    "for i in range(int(len(params))):\n",
    "    \n",
    "    print(f\"1.{i}. fitting the model to my data-set with \", params[i])\n",
    "    cuml_model= cuml_model_est(params[i])\n",
    "    cuml_model.fit(X_train_dask, y_train_dask)\n",
    "    wait(cuml_model.rfs) # Allow asynchronous training tasks to finish\n",
    "\n",
    "    # testing the model\n",
    "    print(f\"2.{i}. validating the model....\")\n",
    "    cuml_y_pred = cuml_model.predict(X_test_dask).compute().to_numpy()\n",
    "\n",
    "    # Due to randomness in the algorithm, you may see slight variation in accuracies\n",
    "    print(f\"3.{i}. cuML accuracy for n_estimators= {params[i]}:  {accuracy_score(y_test, cuml_y_pred):.3f}\")\n",
    "    res_array.append([params[i], accuracy_score(y_test, cuml_y_pred)])\n",
    "\n",
    "print(\"4. all results retrieced by the search:\")\n",
    "res_array\n",
    "# ::::::::::::::::::::::::::::::::\n",
    "# ::::::::::::::::::::::::::::::::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c100c8-1dab-47af-8010-c9ce1b6b3687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def3ce4-4002-44fd-8675-fd679c72fdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39dd1b-e0c0-457e-8180-dddba8011bb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::: trying XGBoost for classification ::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "#   this attempt needs two Nvidia Volta Series GPU\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "# attempts combining cuml.dask with dask_ml did not work out well.\n",
    "# neither make_pipeline nor GridSearchCV did work for two GPUs at the same time\n",
    "\n",
    "print(\"generating cudf based on my existing DataFrames\")\n",
    "# X_train_cudf, X_test_cudf= cudf.DataFrame(X_train).to_cupy(), cudf.DataFrame(X_test).to_cupy()\n",
    "# y_train_cudf, y_test_cudf= cudf.Series(y_train).to_cupy(), cudf.Series(y_test).to_cupy()\n",
    "# X_train_cupy, X_test_cupy= cp.ndarray(X_train), cp.ndarray(X_test)\n",
    "# y_train_cupy, y_test_cupy= cp.array(y_train), cp.arange(y_test)\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Use \"hist\" for constructing the trees, with early stopping enabled.\n",
    "print(\"definition of my classifier\")\n",
    "# clf_cpu = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=2, random_state= 1984, seed= 1984, n_jobs= 16) # this takes about 16 min with 10 cores\n",
    "clf_gpu = xgb.XGBClassifier(device= \"cuda\", tree_method=\"gpu_hist\", early_stopping_rounds=2, random_state= 1984, seed= 1984) # n_gpus= -1,\n",
    "\n",
    "# Fit the model, test sets are used for early stopping.\n",
    "print(\"fitting my classifier to my data-set\")\n",
    "# clf_cpu.fit(X_train_cudf, y_train_cudf, eval_set=[(X_test_cudf, y_test_cudf)])\n",
    "clf_gpu.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d1b0a-76a0-4d6b-8cd6-8c5ac78b1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::: trying XGBoost for classification ::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "#         some hyper parameter optimzation\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "from cuml.model_selection import GridSearchCV as cuml_Grid\n",
    "'''params = {\n",
    "    'learning_rate': [0.03, 0.01],\n",
    "    'min_child_weight': [1,3, 5,7, 10],\n",
    "    'gamma': [0, 0.5, 1, 1.5, 2, 2.5, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9 ,10, 12, 14],\n",
    "    'reg_lambda':np.array([0.4, 0.6, 0.8, 1, 1.2, 1.4])}'''\n",
    "\n",
    "params= {\"learning_rate\": [0.03, 0.01]}\n",
    "\n",
    "# # specific parameters. I set early stopping to avoid overfitting and specify the validation dataset \n",
    "fit_params = { \n",
    "    'early_stopping_rounds':10,\n",
    "    'eval_set':[(X_test, y_test)]}\n",
    "\n",
    "model_gpu= xgb.XGBClassifier(device= \"cuda\", tree_method= \"gpu_hist\", n_gpus= -1, predictor='gpu_predictor', verbose= 10, random_state= 1984, seed= 1984)\n",
    "\n",
    "# # let's run the optimization\n",
    "print(\"running an optimzation scheme based on RandomizedSearCV\")\n",
    "random_search = cuml_Grid(model_gpu, param_grid=params, scoring=\"precision\", n_jobs=2,  verbose=10, cv=4)\n",
    "random_search.fit(X_train,y_train, **fit_params)\n",
    "print(\" Results from Random Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", random_search.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", random_search.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", random_search.best_params_)\n",
    "# :::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14538b2b-eddf-457a-b660-f456f4848ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of theaccurazy of the trained XGBoost-model\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "\n",
    "class_names = pd.Series(y_test).unique\n",
    "# y_predict_cpu= clf_cpu.predict(X_test)\n",
    "y_predict_gpu= clf_gpu.predict(X_test)\n",
    "y_predict_gpu_opt= random_search.predict(X_test)\n",
    "\n",
    "print(\"definition of the image to be generated\")\n",
    "fig, axn = plt.subplots(1,2, figsize= (18,8))\n",
    "\n",
    "sns.heatmap(cm(y_test, y_predict_gpu_opt), annot= True, fmt= \"g\", ax= axn[0]) # cmap=plt.cm.Blues,\n",
    "sns.heatmap(cm(y_test, y_predict_gpu), annot= True, fmt= \"g\", ax= axn[1])\n",
    "\n",
    "[axn[i].set(xlabel= \"prediction\", ylabel= \"truth\") for i in range(2)]\n",
    "plt.tight_layout()\n",
    "axn[0].set_title('opt. GPU Model')\n",
    "axn[1].set_title('GPU Model')\n",
    "plt.show()\n",
    "# :::::::::::::::::::::::::."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b75bb1-8c38-4350-a999-8ea817150715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e0c9b-2410-47af-9de0-de69dc2a9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import accuracy_score\n",
    "from cuml.dask.ensemble import RandomForestClassifier as cumlDaskRF\n",
    "from cuml.ensemble import RandomForestClassifier as cumlRF\n",
    "\n",
    "from cuml.pipeline import make_pipeline as cuml_pipe\n",
    "from sklearn.model_selection import GridSearchCV as sk_Grid\n",
    "from cuml.model_selection import GridSearchCV as cuml_Grid\n",
    "from dask_ml.model_selection import GridSearchCV as dask_Grid\n",
    "from dask_ml.model_selection import IncrementalSearchCV as dask_Inc\n",
    "\n",
    "print(\"definition of the model\")\n",
    "cuml_model= cumlDaskRF(n_estimators=300, n_streams=1, random_state= 1984) # (max_depth=max_depth, n_estimators=n_trees, n_bins=n_bins, n_streams=n_streams)\n",
    "# cuml_model= cumlRF(n_estimator= 300, n_stream= 1, random_state= 1984)\n",
    "\n",
    "params= {\"n_estimators\": [100, 200, 300, 400, 600, 1000]}\n",
    "cuml_model= dask_Grid(estimator= cumlRF(n_streams= 1, random_state= 1984), param_grid= params , cv= 5, scoring= \"r2\") # {\"n_estimators\": [100, 200, 300, 400, 600, 1000]}\n",
    "# cuml_model= dask_Inc(estimator= cumlRF(n_streams= 1, random_state= 1984), parameters= params, scoring= \"r2\")\n",
    "\n",
    "# :::::::::::::::::::::::::::::::::::\n",
    "from dask_ml.wrappers import Incremental\n",
    "cuml_model= Incremental(cuml_model, scoring= \"accuracy\")\n",
    "\n",
    "\n",
    "\n",
    "# :::::::::::::::::::::::::::::::::::\n",
    "# :::::::::::::::::::::::::::::::::::\n",
    "# ## with joblib\n",
    "'''print(\"fitting the model to my data-set by means of using joblib\")\n",
    "from joblib import parallel_backend\n",
    "gs = cuml_Grid(\n",
    "    cuml_model,\n",
    "    params,\n",
    "    cv= 5,\n",
    "    scoring='accuracy',\n",
    "    )\n",
    "with parallel_backend('dask'):\n",
    "    gs.fit(X_train, y_train)'''\n",
    "# :::::::::::::::::::::::::::::::::::\n",
    "# :::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "print(\"fitting the model to my data-set\")\n",
    "cuml_model.fit(X_train_dask, y_train_dask)\n",
    "# cuml_model.fir(X_train_dask, y_train_dask)\n",
    "# wait(cuml_model.rfs) # Allow asynchronous training tasks to finish\n",
    "\n",
    "\n",
    "# testing the model\n",
    "print(\"validating the model....\")\n",
    "cuml_y_pred = cuml_model.predict(X_test_dask).compute().to_numpy()\n",
    "\n",
    "# Due to randomness in the algorithm, you may see slight variation in accuracies\n",
    "print(f\"CuML accuracy:  {accuracy_score(y_test, cuml_y_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53322b97-7925-42ea-8f2b-a4643324207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LogisticRegression as dask_log\n",
    "# dask_Grid(estimator= dask_log(), param_grid= {} , cv= 5, scoring= \"r2\") #.fit(X_train_dask, y_train_dask)\n",
    "\n",
    "\n",
    "dask_log.fit(X_train_dask, y_train_dask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee7f0d-4d83-4694-9cbf-66e64522a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "from dask_ml.preprocessing import StandardScaler as daskStandardScaler\n",
    "from cuml.preprocessing import StandardScaler as cumlStandardScaler\n",
    "from cuml.ensemble import RandomForestClassifier as cumlDaskRF\n",
    "from cuml.dask.ensemble import RandomForestClassifier as daskRF\n",
    "from cuml.pipeline import make_pipeline as cuml_pipe\n",
    "\n",
    "from sklearn.pipeline import make_pipeline as sk_pipe\n",
    "from sklearn.preprocessing import StandardScaler as sk_Scaler\n",
    "from sklearn.ensemble import RandomForestClassifier as sk_RF\n",
    "\n",
    "print(\"defining my pipeline...\")\n",
    "cuml_pipe= cuml_pipe(GarbageCollector(), cumlStandardScaler(), cumlDaskRF(n_estimators= 300, n_streams= 8, random_state= 1984)) # cumlDaskRF(n_estimators= 300, n_streams= 1, random_state= 1984)\n",
    "sk_pipe= sk_pipe(sk_Scaler(), sk_RF(n_estimators= 300, random_state= 1984))\n",
    "\n",
    "pipe= cuml_pipe\n",
    "\n",
    "from joblib import parallel_backend\n",
    "print(\"parallel computing by using joblib\")\n",
    "with parallel_backend(backend= 'dask', workers= workers):\n",
    "    # pipe.fit(X_train, y_train)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "# pipe.fit(X_train, y_train) # running this single command will do the same as joblib.parallel_backend\n",
    "\n",
    "'''clf= cumlDaskRF(n_estimators= 300, random_state= 1984, n_streams= 8)\n",
    "pipe= clf.fit(X_train_dask, y_train_dask) # this works with both GPUs at the same time, but without pipeline'''\n",
    "\n",
    "\n",
    "# testing the model\n",
    "print(\"validating the model....\")\n",
    "cuml_y_pred = pipe.predict(X_test) # .compute().to_numpy()\n",
    "# cuml_y_predict= pipe.predict(X_test_dask).compute().to_numpy()\n",
    "# Due to randomness in the algorithm, you may see slight variation in accuracies\n",
    "print(f\"CuML accuracy:  {accuracy_score(y_test, cuml_y_pred):.3f}\")\n",
    "\n",
    "\n",
    "# ::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803df11-5b5a-40eb-9ab3-67f519e20598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bdc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# definition of the classifiers to be used with CUDA\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "#   this attempt needs two Nvidia Volta Series GPU\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "from cuml.dask.naive_bayes import MultinomialNB as cumlDaskNB\n",
    "from cuml.dask.ensemble import RandomForestClassifier as cumlDaskRF\n",
    "from cuml.dask.linear_model import LogisticRegression as cumlDaskLR\n",
    "from dask_ml.ensemble import BlockwiseVotingClassifier as daskVoting\n",
    "\n",
    "\n",
    "model_params_dask={\n",
    "    \"log_reg\": {\"model\": cumlDaskLR(),\n",
    "           \"params\": {}},\n",
    "    \"naive_bias\": {\"model\": cumlDaskNB(),\n",
    "                  \"params\": {}},\n",
    "    \"random_forest\": {\"model\": cumlDaskRF(n_streams=n_streams, random_state= 1984),\n",
    "                \"params\": {\"randomforestclassiffier__n_estimators\": [100, 200, 300, 500, 1000]\n",
    "                               }},\n",
    "    \"voting\": {\"model\": daskVoting(estimator= cumlDaskLR()),\n",
    "              \"params\": {\"blockwisevotingclassifier__voting\": ['hard', 'soft']}}\n",
    "}\n",
    "\n",
    "\n",
    "import torch\n",
    "print(\"total available CUDA devices: \", torch.cuda.device_count())\n",
    "print(\"active CUDA divice: \", torch.cuda.get_device_name())\n",
    "\n",
    "print(\"::::::::::::::::::::::::::::::::\")\n",
    "print(\":::::::::  finished  :::::::::::\")\n",
    "print(\"::::::::::::::::::::::::::::::::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7294af-dc65-4ea5-8eb7-1018fc911f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca992c-3ab0-47c6-b47e-22bb58fac3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_dask[\"random_forest\"][\"params\"] # .items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e16f8-49d0-4f8f-91d5-eea315fa0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo, mp in model_params_dask.items():\n",
    "    print(mp[\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a16c23f-c4fa-47b0-a551-abe40d2533f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.linear_model import LogisticRegression as daskLog\n",
    "from dask_ml.ensemble import BlockwiseVotingClassifier as daskVoting\n",
    "\n",
    "\n",
    "cumlDaskRF(n_streams=n_streams, random_state= 1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334925e2-53fe-435a-9aec-00f2d24914b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c7ee1-c13f-46b6-94d3-911e73335e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cuml.pipeline import make_pipeline as cumake_pipeline\n",
    "from dask_ml.model_selection import GridSearchCV as daskGridSearchCV\n",
    "from dask_ml.preprocessing import StandardScaler as daskStandardScaler\n",
    "from dask_ml.linear_model import LogisticRegression as daskLog\n",
    "# from cuml.dask.preprocessing import RobustScaler as daskStandardScaler\n",
    "\n",
    "test_model= \"log_reg\"\n",
    "print(\"selected model= \", test_model)\n",
    "\n",
    "print(\"loading the model\")\n",
    "cuml_model= model_params_dask[test_model][\"model\"]\n",
    "\n",
    "print(\"definition of the classier..\")\n",
    "params= model_params_dask[test_model][\"params\"]\n",
    "clf_dask= cumake_pipeline(GarbageCollector(), daskStandardScaler(), cuml_model)\n",
    "clf_cv= daskGridSearchCV(daskLog, param_grid= {}, cv=5, return_train_score=False, scoring='r2')\n",
    "# clf_cv= daskGridSearchCV(clf_dask, params, cv=5, return_train_score=False, scoring='r2')\n",
    "\n",
    "# print(\"scaling the DataFrame\")\n",
    "# X_train_scale= daskStandardScaler().fit_transform(X_train_dask)\n",
    "\n",
    "\n",
    "print(\"fitting the model...\")\n",
    "clf_cv.fit(X_train_dask, y_train_dask) # .astype(np.float32) )\n",
    "# cuml_model.fit(X_train_dask, y_train_dask)\n",
    "# wait(clf_dask.rfs) # Allow asynchronous training tasks to finish\n",
    "\n",
    "# print(clf_cv.best_score_, \" and \", clf_cv.best_params_)\n",
    "\n",
    "# SVC= clf_dask.named_steps[\"truncatedsvd\"] # for pipeline\n",
    "\n",
    "\n",
    "# testing the model\n",
    "print(\"testing the models accuracy...\")\n",
    "cuml_y_pred = clf_cv.predict(X_test_dask).compute().to_numpy()\n",
    "# Due to randomness in the algorithm, you may see slight variation in accuracies\n",
    "print(\"CuML accuracy:     \", accuracy_score(y_test, cuml_y_pred))\n",
    "\n",
    "# for pipeline\n",
    "# cuml_y_pred = SVC.score(X_test_dask, y_test_dask).compute().to_numpy()\n",
    "\n",
    "# Due to randomness in the algorithm, you may see slight variation in accuracies\n",
    "# print(\"CuML accuracy:     \", cuml_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db20d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''model_params_dask={\"XGBoost\":{\"model\": daskXGB(n_streams=n_streams, random_state= 1984, device= \"cuda\"),\n",
    "                              \"params\": {\"n_estimators\": [80, 100, 120, 150, 200]}}\n",
    "                  }'''\n",
    "\n",
    "\n",
    "\n",
    "'''model_params_dask={\n",
    "    \"naive_bias\": {\"model\": daskNB(),\n",
    "                  \"params\": {}},\n",
    "    \"XGBoost\": {\"model\": daskXGB(n_streams=n_streams, random_state= 1984, deveice= \"cuda\"),\n",
    "                \"params\": {\"xgbclassifier__n_estimators\": [80, 100, 120, 150, 200]\n",
    "                               }},\n",
    "    \"lin_reg\": {\"model\": daskLin(max_iter=5000, random_state= 1984),\n",
    "               \"params\": {\"linearregression__algorithm\": [\"eig\", \"SVD\"]\n",
    "                         }},\n",
    "    \"elastic\": {\"model\": daskPR(max_iter=5000, random_state= 1984),\n",
    "                \"params\": {\"poissonregression__C\": [0, 0.3, 0.5, 0.7, 1]\n",
    "                          }}\n",
    "}'''\n",
    "\n",
    "from cuml.dask.ensemble import RandomForestClassifier as daskRF\n",
    "model_params_dask={\n",
    "    \"random\": {\"model\": daskRF(random_state= 1984, n_streams= n_streams),\n",
    "                  \"params\": {\"randomforestclassifier__n_estimators\": [80, 100, 120, 150, 200]}}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7823223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# definition of the classifiers to be used with CUDA\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "#   this attempt needs a Nvidia Volta Series GPU\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "#              refined hyper-parameters\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "model_params_dask={\n",
    "    \"svm\": {\"model\": daskSVC(random_state= 1984),\n",
    "           \"params\": {\"truncatedsvd__n_components\": [1,3,5,7,10],\n",
    "                      \"truncatedsvd__algorithm\": ['tsqr', 'randomized']\n",
    "                     }},\n",
    "    \"naive_bias\": {\"model\": daskNB()},\n",
    "                  \"params\": {}},\n",
    "    \"XGBoost\": {\"model\": daskXGB(n_streams=n_streams, random_state= 1984),\n",
    "                \"params\": {\"xgbclassifier__n_estimators\": [80, 100, 120, 150, 200]\n",
    "                               }}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_dask.shape\n",
    "\n",
    "# X_train_dask.shape\n",
    "\n",
    "# X_train_dask.head(5) #.to_dask_array()\n",
    "\n",
    "pipe_dask\n",
    "\n",
    "# cp.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dee842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::::: looping through the classifiers ::::::::\n",
    "# :::::::::    will run on multiple GPUs    ::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "# generating a pipeline to identify the best performing classification\n",
    "from timeit import default_timer as timer\n",
    "from cuml.pipeline import make_pipeline as cumake_pipeline\n",
    "from dask_ml.model_selection import GridSearchCV as daskGridSearchCV\n",
    "from dask_ml.model_selection import RandomizedSearchCV as daskRandomizedCV\n",
    "# from dask_ml.preprocessing import StandardScaler as daskStandardScaler\n",
    "from dask_ml.preprocessing import MinMaxScaler as daskStandardScaler\n",
    "\n",
    "\n",
    "print(\"starting the timer for measuring the compute time\")\n",
    "start= timer()\n",
    "\n",
    "scores_dask= []\n",
    "best_estimators_dask= {}\n",
    "\n",
    "# transforming my training numpy array into a Dask DataFrame\n",
    "# X_train_dask= dask.dataframe.from_array(X_train)\n",
    "# dask_y_train= dask.dataframe.from_array(y_train)\n",
    "\n",
    "# use \"model_params\" for CPU utilization\n",
    "start= timer()\n",
    "for algo, mp in model_params_dask.items():\n",
    "    print(f\"currently building a model based on {mp['model']} with the following paramters: \\n {mp['params']}\")\n",
    "    # pipe_dask= cumake_pipeline(GarbageCollector(),daskStandardScaler(), mp[\"model\"])\n",
    "    # pipe_dask= cumake_pipeline(GarbageCollector(), mp[\"model\"])\n",
    "    pipe_dask= mp[\"model\"]\n",
    "    clf= daskGridSearchCV(pipe_dask, mp[\"params\"], cv=5, return_train_score=False, scoring='r2', n_jobs= 4) # cv= 5, n_jobs=2\n",
    "    # clf= daskGridSearchCV(daskRF(), param_grid= {\"n_estimators\": [50, 60, 80, 100, 120]}, cv=3, return_train_score=False, scoring='r2')\n",
    "    # clf= daskXGB(n_estimators=200, n_streams=n_streams)\n",
    "    clf.fit(X_train_dask , y_train_dask) # clf.fit(X_train_dask, y_train_dask.to_dask_array())\n",
    "    wait(clf.rfs) # Allow asynchronous training tasks to finish\n",
    "    scores_dask.append({\"model\": mp[\"model\"],\n",
    "        \"best_score\": clf.best_score_,\n",
    "        \"best_params\": clf.best_params_})\n",
    "    best_estimators_dask[algo]= clf.best_estimator_\n",
    "end= timer()\n",
    "\n",
    "print(\":::::::::::::::::::::::::::::::\")\n",
    "print(f\"time it took to find the best params by using the GPU: {(end-start)/60:.3f} minutes\")\n",
    "print(\":::::::::::::::::::::::::::::::\")\n",
    "\n",
    "df_scores_dask= pd.DataFrame(scores_dask, columns=[\"model\", \"best_score\", \"best_params\"])\n",
    "df_scores_dask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# taken from optuna web-site for initiation\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float('x', -10, 10)\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "study.best_params\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Use \"hist\" for constructing the trees, with early stopping enabled.\n",
    "clf = daskXGB(device= \"cuda\", workers= workers, tree_method=\"gpu_hist\", early_stopping_rounds=2, random_state= 1984)\n",
    "# Fit the model, test sets are used for early stopping.\n",
    "clf.fit(X_train_dask, y_train_dask) # , eval_set=[(X_test, y_test)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Use \"hist\" for constructing the trees, with early stopping enabled.\n",
    "clf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=2, random_state= 1984, n_jobs= 16) # this takes about 16 min with 10 cores\n",
    "# Fit the model, test sets are used for early stopping.\n",
    "clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bebe2-4af6-4d25-ab73-9c81a8c769d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e8765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646955f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# :::::::: quick testing if single RFC wortks ::::::\n",
    "# :::::::::    will run on multiple GPUs    ::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "from cuml.metrics import accuracy_score\n",
    "\n",
    "\n",
    "cuml_model = daskXGB(n_estimators=200, n_streams=1)\n",
    "cuml_model.fit(X_train_dask.to_dask_array(), y_train_dask)\n",
    "\n",
    "\n",
    "wait(cuml_model.rfs) # Allow asynchronous training tasks to finish\n",
    "\n",
    "cuml_y_pred = cuml_model.predict(X_test_dask).compute().to_numpy()\n",
    "\n",
    "# Due to randomness in the algorithm, you may see slight variation in accuracies\n",
    "print(f\"CuML accuracy:  {accuracy_score(y_test, cuml_y_pred):.3f}\")\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4add62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2386d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99650797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::\n",
    "# ::::: results based on GPU :::::\n",
    "# ::::::::::::::::::::::::::::::::\n",
    "print(f\"score achived by applying SVC to the train DS: {best_estimators_cu['svm'].score(X_test_cu, y_test_cu):.3f}\")\n",
    "print(f\"score achived by applying GNB to the train DS: {best_estimators_cu['naive_bias'].score(X_test_cu, y_test_cu):.3f}\")\n",
    "print(f\"score achived by applying XGB to the train DS: {best_estimators_cu['XGBoost'].score(X_test_cu, y_test_cu):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e145250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# :::::::::: generation of confusino matrix ::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "import seaborn as sns\n",
    "\n",
    "_class=[\"svm\", \"svm_lin\", \"rand_forest\", \"log_reg\"]\n",
    "# defining an array with the classifiers containing their best estimators\n",
    "clf= [best_estimators_cu[_class[i]] for i in range(4)]\n",
    "\n",
    "# looping throught the different classiffiers\n",
    "y_predict= [clf[i].predict(X_test_cu) for i in range(4)]\n",
    "\n",
    "\n",
    "# generating confucion matrices for the different classifies\n",
    "cm_PCB= [cm(y_test, y_predict[i].get()) for i in range(4)]\n",
    "\n",
    "fig, axm= plt.subplots(2,2, figsize= (10,10), sharex=True, sharey=True)\n",
    "\n",
    "sns.heatmap(cm_PCB[0], annot=True, ax= axm[0,0])\n",
    "sns.heatmap(cm_PCB[1], annot=True, ax= axm[0,1])\n",
    "sns.heatmap(cm_PCB[2], annot=True, ax= axm[1,0])\n",
    "sns.heatmap(cm_PCB[3], annot=True, ax= axm[1,1])\n",
    "\n",
    "for ax in axm.flat:\n",
    "    # set common labels\n",
    "    ax.set(xlabel= \"prediction\", ylabel= \"truth\")\n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    ax.label_outer()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# ::::::::::: exporting my optimzed models :::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "import pickle\n",
    "\n",
    "# looping through all models defined and optimzed\n",
    "[pickle.dump(clf[i], open(directory_settings+ _class[i]+\"_dask_cv5_4bit_model.pkl\", \"wb\")) for i in range(int(len(_class)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9128f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518aecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261e22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a04a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b760bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   !!! skip it for the time being !!!\n",
    "''' !!! skip it for the time being !!! '''\n",
    "\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# :::::::::: definition of the classifiers :::::::::\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import GridSearchCV as GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler as StandardScaler\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "from joblib import parallel_backend\n",
    "# ::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "scores= []\n",
    "best_estimators= {}\n",
    "\n",
    "# use \"model_params\" for CPU utilization\n",
    "start= timer()\n",
    "with parallel_backend('threading', n_jobs=2):\n",
    "    for algo, mp in model_params.items():\n",
    "        print(f\"currently building a model based on {mp['model']} with the following paramters: \\n {mp['params']}\")\n",
    "        pipe= make_pipeline(StandardScaler(), mp[\"model\"])\n",
    "        clf= GridSearchCV(pipe, mp[\"params\"], cv=5, return_train_score=False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append({\"model\": mp[\"model\"],\n",
    "            \"best_score\": clf.best_score_,\n",
    "            \"best_params\": clf.best_params_})\n",
    "        best_estimators[algo]= clf.best_estimator_\n",
    "end= timer()\n",
    "\n",
    "print(\":::::::::::::::::::::::::::::::\")\n",
    "print(f\"time it took to find the best params by using the CPU: {(end-start):.3f}\")\n",
    "print(\":::::::::::::::::::::::::::::::\")\n",
    "\n",
    "df_scores= pd.DataFrame(scores, columns=[\"model\", \"best_score\", \"best_params\"])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
